{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "from sklearn.model_selection import GroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_kfolds = 5\n",
    "batch_size = 32\n",
    "learning_rate = 3e-3\n",
    "num_epoch = 10\n",
    "es_patience = 20\n",
    "quantiles = (0.2, 0.5, 0.8)\n",
    "model_name = 'descartes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Metric Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_log_likelihood(actual_FVC, predicted_FVC, confidence, return_values = False):\n",
    "    sd_clipped = np.maximum(confidence, 70)\n",
    "    delta = np.minimum(np.abs(actual_FVC - predicted_FVC), 1000)\n",
    "    metric = -np.sqrt(2) * delta / sd_clipped - np.log(np.sqrt(2) * sd_clipped)\n",
    "    \n",
    "    if return_values:\n",
    "        return metric\n",
    "    else:\n",
    "        return np.mean(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalDataset(Dataset):\n",
    "    def __init__(self, mode, transform=None):\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        \n",
    "        train = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/train.csv')\n",
    "        \n",
    "        # Some of the patients have inspected twice a week. So, remove duplicated records. \n",
    "        train.drop_duplicates(keep=False, inplace=True, subset=['Patient', 'Weeks'])\n",
    "        \n",
    "        sub = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/sample_submission.csv')\n",
    "        sub['Patient'] = sub['Patient_Week'].apply(lambda x: x.split('_')[0])\n",
    "        sub['Weeks'] = sub['Patient_Week'].apply(lambda x: int(x.split('_')[-1]))\n",
    "        sub = sub[['Patient', 'Weeks', 'Confidence', 'Patient_Week']]\n",
    "        \n",
    "        chunk = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/test.csv')\n",
    "        sub = sub.merge(chunk.drop('Weeks', axis=1), on='Patient')\n",
    "        \n",
    "        train['WHERE'] = 'train'\n",
    "        chunk['WHERE'] = 'val'\n",
    "        sub['WHERE'] = 'test'\n",
    "        \n",
    "        # Merge train, test, test data.\n",
    "        data = train.append([chunk, sub])\n",
    "        \n",
    "        # Calculate min_week by each patient.\n",
    "        data['min_week'] = data['Weeks']\n",
    "        data.loc[data.WHERE == 'test', 'min_week'] = np.nan\n",
    "        data['min_week'] = data.groupby('Patient')['min_week'].transform('min')\n",
    "        \n",
    "        init_exam = data.loc[data.Weeks == data.min_week]\n",
    "        init_exam = init_exam[['Patient', 'FVC']].copy()\n",
    "        init_exam.columns = ['Patient', 'min_FVC']\n",
    "        init_exam['nb'] = 1\n",
    "        init_exam['nb'] = init_exam.groupby('Patient')['nb'].transform('cumsum')\n",
    "        init_exam = init_exam[init_exam.nb == 1]\n",
    "        init_exam.drop('nb', axis=1, inplace=True)\n",
    "        \n",
    "        # Calculate day\n",
    "        data = data.merge(init_exam, on='Patient', how='left')\n",
    "        data['base_week'] = data['Weeks'] - data['min_week']\n",
    "        del init_exam\n",
    "        \n",
    "        # Transform 'Sex' and 'SmokingStatus' data into One-Hot vector.\n",
    "        COLS = ['Sex', 'SmokingStatus']\n",
    "        self.FE = []\n",
    "        for col in COLS:\n",
    "            for mod in data[col].unique():\n",
    "                self.FE.append(mod)\n",
    "                data[mod] = (data[col] == mod).astype(int)\n",
    "        \n",
    "        # Normalize each data so that the maximum value is 1 and the minimum value is 0.\n",
    "        data['age'] = (data['Age'] - data['Age'].min()) / \\\n",
    "                      (data['Age'].max() - data['Age'].min())\n",
    "        \n",
    "        data['BASE'] = (data['min_FVC'] - data['min_FVC'].min()) / \\\n",
    "                      (data['min_FVC'].max() - data['min_FVC'].min())\n",
    "\n",
    "        data['week'] = (data['base_week'] - data['base_week'].min()) / \\\n",
    "                      (data['base_week'].max() - data['base_week'].min())\n",
    "\n",
    "        data['percent'] = (data['Percent'] - data['Percent'].min()) / \\\n",
    "                      (data['Percent'].max() - data['Percent'].min())\n",
    "        \n",
    "        self.FE += ['age', 'percent', 'week', 'BASE']\n",
    "        self.raw = data.loc[data.WHERE == mode].reset_index()\n",
    "        del data\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.raw)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        sample = {\n",
    "            'patient_id': self.raw['Patient'].iloc[idx],\n",
    "            'features': self.raw[self.FE].iloc[idx].values,\n",
    "            'target': self.raw['FVC'].iloc[idx]\n",
    "        }\n",
    "        if self.transform:\n",
    "            sample = transform(sample)\n",
    "    \n",
    "        return sample\n",
    "    \n",
    "    \n",
    "    def group_kfold(self, n_splits):\n",
    "        gkf = GroupKFold(n_splits=n_splits)\n",
    "        groups = self.raw['Patient']\n",
    "        for train_idx, val_idx in gkf.split(self.raw, self.raw, groups):\n",
    "            train = Subset(self, train_idx)\n",
    "            val = Subset(self, val_idx)\n",
    "            yield train, val\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model (Quantile Regression Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QrnnModel(nn.Module):\n",
    "    def __init__(self, in_tabular_features=9, out_quantiles=3):\n",
    "        super(QrnnModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_tabular_features, 100)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=100)\n",
    "        self.fc2 = nn.Linear(100, 100)\n",
    "        self.fc3 = nn.Linear(100, out_quantiles)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "def quantile_loss(preds, target, quantiles):\n",
    "    losses = []\n",
    "    for i, q in enumerate(quantiles):\n",
    "        errors = target - preds[:, i]\n",
    "        losses.append(torch.max((q - 1) * errors, q * errors).unsqueeze(1))\n",
    "    loss = torch.mean(torch.sum(torch.cat(losses, dim=1), dim=1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************** Fold No.[0] ********************\n",
      "Epoch #1, Iteration #10, loss: 4065.20458984375\n",
      "Epoch #1, Iteration #20, loss: 4415.58447265625\n",
      "Epoch #1, Iteration #30, loss: 3869.9677734375\n",
      "Epoch #1, Metric loss: -24.798133850097656\n",
      "\n",
      "Epoch #2, Iteration #10, loss: 4630.5322265625\n",
      "Epoch #2, Iteration #20, loss: 3930.24951171875\n",
      "Epoch #2, Iteration #30, loss: 3316.669677734375\n",
      "Epoch #2, Metric loss: -24.480541229248047\n",
      "\n",
      "Epoch #3, Iteration #10, loss: 3167.162109375\n",
      "Epoch #3, Iteration #20, loss: 2877.486328125\n",
      "Epoch #3, Iteration #30, loss: 1942.4110107421875\n",
      "Epoch #3, Metric loss: -20.11869239807129\n",
      "\n",
      "Epoch #4, Iteration #10, loss: 1783.5174560546875\n",
      "Epoch #4, Iteration #20, loss: 1671.781494140625\n",
      "Epoch #4, Iteration #30, loss: 708.0923461914062\n",
      "Epoch #4, Metric loss: -8.514203071594238\n",
      "\n",
      "Epoch #5, Iteration #10, loss: 581.7266845703125\n",
      "Epoch #5, Iteration #20, loss: 1008.4696044921875\n",
      "Epoch #5, Iteration #30, loss: 437.4254150390625\n",
      "Epoch #5, Metric loss: -7.627336025238037\n",
      "\n",
      "Epoch #6, Iteration #10, loss: 473.74749755859375\n",
      "Epoch #6, Iteration #20, loss: 310.720703125\n",
      "Epoch #6, Iteration #30, loss: 639.2713623046875\n",
      "Epoch #6, Metric loss: -7.375636100769043\n",
      "\n",
      "Epoch #7, Iteration #10, loss: 278.4224548339844\n",
      "Epoch #7, Iteration #20, loss: 1101.021484375\n",
      "Epoch #7, Iteration #30, loss: 377.3927917480469\n",
      "Epoch #7, Metric loss: -7.403709888458252\n",
      "\n",
      "Epoch #8, Iteration #10, loss: 285.37066650390625\n",
      "Epoch #8, Iteration #20, loss: 391.85003662109375\n",
      "Epoch #8, Iteration #30, loss: 330.549072265625\n",
      "Epoch #8, Metric loss: -7.2861104011535645\n",
      "\n",
      "Epoch #9, Iteration #10, loss: 1064.296630859375\n",
      "Epoch #9, Iteration #20, loss: 439.3774719238281\n",
      "Epoch #9, Iteration #30, loss: 391.45404052734375\n",
      "Epoch #9, Metric loss: -7.542897701263428\n",
      "\n",
      "Epoch #10, Iteration #10, loss: 399.02752685546875\n",
      "Epoch #10, Iteration #20, loss: 706.8099365234375\n",
      "Epoch #10, Iteration #30, loss: 401.2035217285156\n",
      "Epoch #10, Metric loss: -7.487602233886719\n",
      "\n",
      "***************** Fold No.[1] ********************\n",
      "Epoch #1, Iteration #10, loss: 3969.752685546875\n",
      "Epoch #1, Iteration #20, loss: 4084.1376953125\n",
      "Epoch #1, Iteration #30, loss: 3944.34423828125\n",
      "Epoch #1, Metric loss: -24.783803939819336\n",
      "\n",
      "Epoch #2, Iteration #10, loss: 4247.22607421875\n",
      "Epoch #2, Iteration #20, loss: 3562.366943359375\n",
      "Epoch #2, Iteration #30, loss: 3139.449462890625\n",
      "Epoch #2, Metric loss: -24.529712677001953\n",
      "\n",
      "Epoch #3, Iteration #10, loss: 3041.21923828125\n",
      "Epoch #3, Iteration #20, loss: 2533.411376953125\n",
      "Epoch #3, Iteration #30, loss: 2145.59765625\n",
      "Epoch #3, Metric loss: -18.542076110839844\n",
      "\n",
      "Epoch #4, Iteration #10, loss: 1245.0906982421875\n",
      "Epoch #4, Iteration #20, loss: 859.5269165039062\n",
      "Epoch #4, Iteration #30, loss: 851.6836547851562\n",
      "Epoch #4, Metric loss: -8.070233345031738\n",
      "\n",
      "Epoch #5, Iteration #10, loss: 990.6319580078125\n",
      "Epoch #5, Iteration #20, loss: 1211.05810546875\n",
      "Epoch #5, Iteration #30, loss: 571.8834228515625\n",
      "Epoch #5, Metric loss: -7.85662317276001\n",
      "\n",
      "Epoch #6, Iteration #10, loss: 924.31494140625\n",
      "Epoch #6, Iteration #20, loss: 482.50799560546875\n",
      "Epoch #6, Iteration #30, loss: 484.71142578125\n",
      "Epoch #6, Metric loss: -7.674050807952881\n",
      "\n",
      "Epoch #7, Iteration #10, loss: 656.1766357421875\n",
      "Epoch #7, Iteration #20, loss: 701.9442138671875\n",
      "Epoch #7, Iteration #30, loss: 802.5741577148438\n",
      "Epoch #7, Metric loss: -7.690145015716553\n",
      "\n",
      "Epoch #8, Iteration #10, loss: 772.78955078125\n",
      "Epoch #8, Iteration #20, loss: 583.9583740234375\n",
      "Epoch #8, Iteration #30, loss: 367.643310546875\n",
      "Epoch #8, Metric loss: -7.569162845611572\n",
      "\n",
      "Epoch #9, Iteration #10, loss: 341.0002136230469\n",
      "Epoch #9, Iteration #20, loss: 856.8646240234375\n",
      "Epoch #9, Iteration #30, loss: 455.02923583984375\n",
      "Epoch #9, Metric loss: -7.6838297843933105\n",
      "\n",
      "Epoch #10, Iteration #10, loss: 583.8558349609375\n",
      "Epoch #10, Iteration #20, loss: 309.3680114746094\n",
      "Epoch #10, Iteration #30, loss: 875.6317138671875\n",
      "Epoch #10, Metric loss: -7.534449577331543\n",
      "\n",
      "***************** Fold No.[2] ********************\n",
      "Epoch #1, Iteration #10, loss: 3939.537841796875\n",
      "Epoch #1, Iteration #20, loss: 4051.938232421875\n",
      "Epoch #1, Iteration #30, loss: 4031.9541015625\n",
      "Epoch #1, Metric loss: -24.798133850097656\n",
      "\n",
      "Epoch #2, Iteration #10, loss: 3801.0048828125\n",
      "Epoch #2, Iteration #20, loss: 3759.59814453125\n",
      "Epoch #2, Iteration #30, loss: 3423.9326171875\n",
      "Epoch #2, Metric loss: -24.0064697265625\n",
      "\n",
      "Epoch #3, Iteration #10, loss: 3118.207763671875\n",
      "Epoch #3, Iteration #20, loss: 2636.3994140625\n",
      "Epoch #3, Iteration #30, loss: 2207.86083984375\n",
      "Epoch #3, Metric loss: -17.654354095458984\n",
      "\n",
      "Epoch #4, Iteration #10, loss: 1707.91796875\n",
      "Epoch #4, Iteration #20, loss: 995.8214721679688\n",
      "Epoch #4, Iteration #30, loss: 899.5615234375\n",
      "Epoch #4, Metric loss: -8.00556755065918\n",
      "\n",
      "Epoch #5, Iteration #10, loss: 325.52960205078125\n",
      "Epoch #5, Iteration #20, loss: 776.8868408203125\n",
      "Epoch #5, Iteration #30, loss: 386.8749694824219\n",
      "Epoch #5, Metric loss: -7.496785640716553\n",
      "\n",
      "Epoch #6, Iteration #10, loss: 594.9920654296875\n",
      "Epoch #6, Iteration #20, loss: 510.67742919921875\n",
      "Epoch #6, Iteration #30, loss: 415.0719909667969\n",
      "Epoch #6, Metric loss: -7.495884895324707\n",
      "\n",
      "Epoch #7, Iteration #10, loss: 486.76385498046875\n",
      "Epoch #7, Iteration #20, loss: 742.4024658203125\n",
      "Epoch #7, Iteration #30, loss: 1054.270751953125\n",
      "Epoch #7, Metric loss: -7.515416145324707\n",
      "\n",
      "Epoch #8, Iteration #10, loss: 1074.8511962890625\n",
      "Epoch #8, Iteration #20, loss: 298.882568359375\n",
      "Epoch #8, Iteration #30, loss: 284.91204833984375\n",
      "Epoch #8, Metric loss: -7.472053050994873\n",
      "\n",
      "Epoch #9, Iteration #10, loss: 471.48748779296875\n",
      "Epoch #9, Iteration #20, loss: 324.4431457519531\n",
      "Epoch #9, Iteration #30, loss: 277.8937072753906\n",
      "Epoch #9, Metric loss: -7.273126125335693\n",
      "\n",
      "Epoch #10, Iteration #10, loss: 274.93023681640625\n",
      "Epoch #10, Iteration #20, loss: 312.1379699707031\n",
      "Epoch #10, Iteration #30, loss: 478.2307434082031\n",
      "Epoch #10, Metric loss: -7.402334213256836\n",
      "\n",
      "***************** Fold No.[3] ********************\n",
      "Epoch #1, Iteration #10, loss: 4066.494384765625\n",
      "Epoch #1, Iteration #20, loss: 4280.42578125\n",
      "Epoch #1, Iteration #30, loss: 3631.330078125\n",
      "Epoch #1, Metric loss: -24.727476119995117\n",
      "\n",
      "Epoch #2, Iteration #10, loss: 4010.1083984375\n",
      "Epoch #2, Iteration #20, loss: 3946.8232421875\n",
      "Epoch #2, Iteration #30, loss: 3907.323974609375\n",
      "Epoch #2, Metric loss: -24.276037216186523\n",
      "\n",
      "Epoch #3, Iteration #10, loss: 2875.6923828125\n",
      "Epoch #3, Iteration #20, loss: 2298.713134765625\n",
      "Epoch #3, Iteration #30, loss: 1855.2183837890625\n",
      "Epoch #3, Metric loss: -14.969266891479492\n",
      "\n",
      "Epoch #4, Iteration #10, loss: 822.250732421875\n",
      "Epoch #4, Iteration #20, loss: 646.6948852539062\n",
      "Epoch #4, Iteration #30, loss: 854.9417114257812\n",
      "Epoch #4, Metric loss: -7.557997703552246\n",
      "\n",
      "Epoch #5, Iteration #10, loss: 576.4380493164062\n",
      "Epoch #5, Iteration #20, loss: 377.61724853515625\n",
      "Epoch #5, Iteration #30, loss: 425.246337890625\n",
      "Epoch #5, Metric loss: -7.201857089996338\n",
      "\n",
      "Epoch #6, Iteration #10, loss: 301.3761291503906\n",
      "Epoch #6, Iteration #20, loss: 369.63311767578125\n",
      "Epoch #6, Iteration #30, loss: 462.52008056640625\n",
      "Epoch #6, Metric loss: -7.3718109130859375\n",
      "\n",
      "Epoch #7, Iteration #10, loss: 806.3553466796875\n",
      "Epoch #7, Iteration #20, loss: 505.9587097167969\n",
      "Epoch #7, Iteration #30, loss: 389.31719970703125\n",
      "Epoch #7, Metric loss: -7.520830154418945\n",
      "\n",
      "Epoch #8, Iteration #10, loss: 358.8238525390625\n",
      "Epoch #8, Iteration #20, loss: 349.79656982421875\n",
      "Epoch #8, Iteration #30, loss: 312.04632568359375\n",
      "Epoch #8, Metric loss: -7.416587829589844\n",
      "\n",
      "Epoch #9, Iteration #10, loss: 247.95864868164062\n",
      "Epoch #9, Iteration #20, loss: 380.939697265625\n",
      "Epoch #9, Iteration #30, loss: 339.3660888671875\n",
      "Epoch #9, Metric loss: -7.249504566192627\n",
      "\n",
      "Epoch #10, Iteration #10, loss: 258.30767822265625\n",
      "Epoch #10, Iteration #20, loss: 373.6502380371094\n",
      "Epoch #10, Iteration #30, loss: 329.76214599609375\n",
      "Epoch #10, Metric loss: -7.302042007446289\n",
      "\n",
      "***************** Fold No.[4] ********************\n",
      "Epoch #1, Iteration #10, loss: 4164.37548828125\n",
      "Epoch #1, Iteration #20, loss: 3816.8544921875\n",
      "Epoch #1, Iteration #30, loss: 3940.71337890625\n",
      "Epoch #1, Metric loss: -24.762527465820312\n",
      "\n",
      "Epoch #2, Iteration #10, loss: 3735.9267578125\n",
      "Epoch #2, Iteration #20, loss: 3990.9169921875\n",
      "Epoch #2, Iteration #30, loss: 3524.087890625\n",
      "Epoch #2, Metric loss: -24.03342056274414\n",
      "\n",
      "Epoch #3, Iteration #10, loss: 3154.011962890625\n",
      "Epoch #3, Iteration #20, loss: 2151.94140625\n",
      "Epoch #3, Iteration #30, loss: 1689.81640625\n",
      "Epoch #3, Metric loss: -15.469274520874023\n",
      "\n",
      "Epoch #4, Iteration #10, loss: 1151.859619140625\n",
      "Epoch #4, Iteration #20, loss: 643.5810546875\n",
      "Epoch #4, Iteration #30, loss: 441.2763366699219\n",
      "Epoch #4, Metric loss: -7.705356597900391\n",
      "\n",
      "Epoch #5, Iteration #10, loss: 1062.41064453125\n",
      "Epoch #5, Iteration #20, loss: 342.2979736328125\n",
      "Epoch #5, Iteration #30, loss: 1790.518310546875\n",
      "Epoch #5, Metric loss: -7.502867698669434\n",
      "\n",
      "Epoch #6, Iteration #10, loss: 508.4434814453125\n",
      "Epoch #6, Iteration #20, loss: 341.2467956542969\n",
      "Epoch #6, Iteration #30, loss: 376.23431396484375\n",
      "Epoch #6, Metric loss: -7.36639404296875\n",
      "\n",
      "Epoch #7, Iteration #10, loss: 367.5190124511719\n",
      "Epoch #7, Iteration #20, loss: 289.5757751464844\n",
      "Epoch #7, Iteration #30, loss: 282.59527587890625\n",
      "Epoch #7, Metric loss: -7.3741326332092285\n",
      "\n",
      "Epoch #8, Iteration #10, loss: 435.1087646484375\n",
      "Epoch #8, Iteration #20, loss: 235.73519897460938\n",
      "Epoch #8, Iteration #30, loss: 322.9391174316406\n",
      "Epoch #8, Metric loss: -7.244561195373535\n",
      "\n",
      "Epoch #9, Iteration #10, loss: 498.50506591796875\n",
      "Epoch #9, Iteration #20, loss: 269.1004638671875\n",
      "Epoch #9, Iteration #30, loss: 407.7344970703125\n",
      "Epoch #9, Metric loss: -7.360838413238525\n",
      "\n",
      "Epoch #10, Iteration #10, loss: 286.4328918457031\n",
      "Epoch #10, Iteration #20, loss: 1332.47802734375\n",
      "Epoch #10, Iteration #30, loss: 365.69537353515625\n",
      "Epoch #10, Metric loss: -7.3038129806518555\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "data = MedicalDataset(mode='train')\n",
    "folds = data.group_kfold(num_kfolds)\n",
    "\n",
    "for fold, (trainset, valset) in enumerate(folds):\n",
    "    print(\"***************** Fold No.{} ********************\".format([fold]))\n",
    "    \n",
    "    dataloaders = {\n",
    "        'train': DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2),\n",
    "        'val': DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    }\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = QrnnModel().to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        \n",
    "        all_pred = []\n",
    "        all_target = []\n",
    "        all_loss = []\n",
    "        all_metrix_loss = 0\n",
    "        \n",
    "        # train\n",
    "        itr = 1\n",
    "        for batch in dataloaders['train']:\n",
    "            model.train()\n",
    "            inputs = batch['features'].float().to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                preds = model(inputs)\n",
    "                loss = quantile_loss(preds, targets, quantiles)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                all_loss.append(loss)\n",
    "\n",
    "            if itr % 10 == 0:\n",
    "                print(f\"Epoch #{epoch+1}, Iteration #{itr}, loss: {loss}\")\n",
    "            itr += 1\n",
    "\n",
    "        # validation\n",
    "        for batch in dataloaders['val']:\n",
    "            model.eval()\n",
    "            inputs = batch['features'].float().to(device)\n",
    "            target = batch['target']\n",
    "            optimizer.zero_grad()\n",
    "            with torch.set_grad_enabled(False):\n",
    "                preds = model(inputs)\n",
    "                all_pred.extend(preds.detach().cpu().numpy().tolist())\n",
    "                all_target.extend(target.numpy().tolist())\n",
    "        \n",
    "        all_pred = torch.FloatTensor(all_pred)\n",
    "        all_target = torch.FloatTensor(all_target)\n",
    "        all_target=torch.reshape(all_target, all_pred[:,1].shape)\n",
    "        all_confidence = all_pred[:, 2] - all_pred[:, 0]\n",
    "        metric_loss = laplace_log_likelihood(all_pred[:, 1], all_target, all_confidence, True)\n",
    "\n",
    "        print(f\"Epoch #{epoch+1}, Metric loss: {metric_loss.mean()}\\n\")\n",
    "            \n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MedicalDataset(mode='test')\n",
    "avg_preds = np.zeros((len(data), len(quantiles)))\n",
    "                     \n",
    "for model in models:\n",
    "    dataloader = DataLoader(data, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    preds = []\n",
    "    for batch in dataloader:\n",
    "        inputs = batch['features'].float()\n",
    "        with torch.no_grad():\n",
    "            x = model(inputs)\n",
    "            preds.append(x)\n",
    "            \n",
    "    preds = torch.cat(preds, dim=0).numpy()\n",
    "    avg_preds += preds\n",
    "    \n",
    "avg_preds /= len(models)\n",
    "df = pd.DataFrame(data = avg_preds, columns=list(quantiles))\n",
    "df['Patient_Week'] = data.raw['Patient_Week']\n",
    "df['FVC'] = df[quantiles[1]]\n",
    "df['Confidence'] = df[quantiles[2]] - df[quantiles[0]]\n",
    "df = df.drop(columns=list(quantiles))\n",
    "df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
