{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "from sklearn.model_selection import GroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_kfolds = 5\n",
    "batch_size = 32\n",
    "learning_rate = 3e-3\n",
    "num_epoch = 10\n",
    "es_patience = 20\n",
    "quantiles = (0.2, 0.5, 0.8)\n",
    "model_name = 'descartes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Metric Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_log_likelihood(actual_FVC, predicted_FVC, confidence, return_values = False):\n",
    "    sd_clipped = np.maximum(confidence, 70)\n",
    "    delta = np.minimum(np.abs(actual_FVC - predicted_FVC), 1000)\n",
    "    metric = -np.sqrt(2) * delta / sd_clipped - np.log(np.sqrt(2) * sd_clipped)\n",
    "    \n",
    "    if return_values:\n",
    "        return metric\n",
    "    else:\n",
    "        return np.mean(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalDataset(Dataset):\n",
    "    def __init__(self, mode, transform=None):\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        \n",
    "        train = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/train.csv')\n",
    "        \n",
    "        # Some of the patients have inspected twice a week. So, remove duplicated records. \n",
    "        train.drop_duplicates(keep=False, inplace=True, subset=['Patient', 'Weeks'])\n",
    "        \n",
    "        sub = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/sample_submission.csv')\n",
    "        sub['Patient'] = sub['Patient_Week'].apply(lambda x: x.split('_')[0])\n",
    "        sub['Weeks'] = sub['Patient_Week'].apply(lambda x: int(x.split('_')[-1]))\n",
    "        sub = sub[['Patient', 'Weeks', 'Confidence', 'Patient_Week']]\n",
    "        \n",
    "        chunk = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/test.csv')\n",
    "        sub = sub.merge(chunk.drop('Weeks', axis=1), on='Patient')\n",
    "        \n",
    "        train['WHERE'] = 'train'\n",
    "        chunk['WHERE'] = 'val'\n",
    "        sub['WHERE'] = 'test'\n",
    "        \n",
    "        # Merge train, test, test data.\n",
    "        data = train.append([chunk, sub])\n",
    "        \n",
    "        # Calculate min_week by each patient.\n",
    "        data['min_week'] = data['Weeks']\n",
    "        data.loc[data.WHERE == 'test', 'min_week'] = np.nan\n",
    "        data['min_week'] = data.groupby('Patient')['min_week'].transform('min')\n",
    "        \n",
    "        init_exam = data.loc[data.Weeks == data.min_week]\n",
    "        init_exam = init_exam[['Patient', 'FVC']].copy()\n",
    "        init_exam.columns = ['Patient', 'min_FVC']\n",
    "        init_exam['nb'] = 1\n",
    "        init_exam['nb'] = init_exam.groupby('Patient')['nb'].transform('cumsum')\n",
    "        init_exam = init_exam[init_exam.nb == 1]\n",
    "        init_exam.drop('nb', axis=1, inplace=True)\n",
    "        \n",
    "        # Calculate day\n",
    "        data = data.merge(init_exam, on='Patient', how='left')\n",
    "        data['base_week'] = data['Weeks'] - data['min_week']\n",
    "        del init_exam\n",
    "        \n",
    "        # Transform 'Sex' and 'SmokingStatus' data into One-Hot vector.\n",
    "        COLS = ['Sex', 'SmokingStatus']\n",
    "        self.FE = []\n",
    "        for col in COLS:\n",
    "            for mod in data[col].unique():\n",
    "                self.FE.append(mod)\n",
    "                data[mod] = (data[col] == mod).astype(int)\n",
    "        \n",
    "        # Normalize each data so that the maximum value is 1 and the minimum value is 0.\n",
    "        data['age'] = (data['Age'] - data['Age'].min()) / \\\n",
    "                      (data['Age'].max() - data['Age'].min())\n",
    "        \n",
    "        data['BASE'] = (data['min_FVC'] - data['min_FVC'].min()) / \\\n",
    "                      (data['min_FVC'].max() - data['min_FVC'].min())\n",
    "\n",
    "        data['week'] = (data['base_week'] - data['base_week'].min()) / \\\n",
    "                      (data['base_week'].max() - data['base_week'].min())\n",
    "\n",
    "        data['percent'] = (data['Percent'] - data['Percent'].min()) / \\\n",
    "                      (data['Percent'].max() - data['Percent'].min())\n",
    "        \n",
    "        self.FE += ['age', 'percent', 'week', 'BASE']\n",
    "        self.raw = data.loc[data.WHERE == mode].reset_index()\n",
    "        del data\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.raw)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        sample = {\n",
    "            'patient_id': self.raw['Patient'].iloc[idx],\n",
    "            'features': self.raw[self.FE].iloc[idx].values,\n",
    "            'target': self.raw['FVC'].iloc[idx]\n",
    "        }\n",
    "        if self.transform:\n",
    "            sample = transform(sample)\n",
    "    \n",
    "        return sample\n",
    "    \n",
    "    \n",
    "    def group_kfold(self, n_splits):\n",
    "        gkf = GroupKFold(n_splits=n_splits)\n",
    "        groups = self.raw['Patient']\n",
    "        for train_idx, val_idx in gkf.split(self.raw, self.raw, groups):\n",
    "            train = Subset(self, train_idx)\n",
    "            val = Subset(self, val_idx)\n",
    "            yield train, val\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model (Quantile Regression Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QrnnModel(nn.Module):\n",
    "    def __init__(self, in_tabular_features=9, out_quantiles=3):\n",
    "        super(QrnnModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_tabular_features, 100)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=100)\n",
    "        self.fc2 = nn.Linear(100, 100)\n",
    "        self.fc3 = nn.Linear(100, out_quantiles)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "def quantile_loss(preds, target, quantiles):\n",
    "    losses = []\n",
    "    for i, q in enumerate(quantiles):\n",
    "        errors = target - preds[:, i]\n",
    "        losses.append(torch.max((q - 1) * errors, q * errors).unsqueeze(1))\n",
    "    loss = torch.mean(torch.sum(torch.cat(losses, dim=1), dim=1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************** Fold No.[0] ********************\n",
      "Epoch #1, Iteration #10, loss: 4146.05859375\n",
      "Epoch #1, Iteration #20, loss: 4119.82470703125\n",
      "Epoch #1, Iteration #30, loss: 4186.79736328125\n",
      "Epoch #1, Metric loss: -24.798133850097656\n",
      "\n",
      "Epoch #2, Iteration #10, loss: 3734.156494140625\n",
      "Epoch #2, Iteration #20, loss: 3730.788818359375\n",
      "Epoch #2, Iteration #30, loss: 3679.80908203125\n",
      "Epoch #2, Metric loss: -24.65211296081543\n",
      "\n",
      "Epoch #3, Iteration #10, loss: 3177.709716796875\n",
      "Epoch #3, Iteration #20, loss: 2385.972900390625\n",
      "Epoch #3, Iteration #30, loss: 1786.61279296875\n",
      "Epoch #3, Metric loss: -15.424593925476074\n",
      "\n",
      "Epoch #4, Iteration #10, loss: 1748.4720458984375\n",
      "Epoch #4, Iteration #20, loss: 513.23388671875\n",
      "Epoch #4, Iteration #30, loss: 317.7638244628906\n",
      "Epoch #4, Metric loss: -7.435503959655762\n",
      "\n",
      "Epoch #5, Iteration #10, loss: 782.9888916015625\n",
      "Epoch #5, Iteration #20, loss: 554.678466796875\n",
      "Epoch #5, Iteration #30, loss: 349.2149963378906\n",
      "Epoch #5, Metric loss: -7.400668144226074\n",
      "\n",
      "Epoch #6, Iteration #10, loss: 312.3706359863281\n",
      "Epoch #6, Iteration #20, loss: 323.8208312988281\n",
      "Epoch #6, Iteration #30, loss: 332.7859191894531\n",
      "Epoch #6, Metric loss: -7.264014720916748\n",
      "\n",
      "Epoch #7, Iteration #10, loss: 620.4498291015625\n",
      "Epoch #7, Iteration #20, loss: 401.21636962890625\n",
      "Epoch #7, Iteration #30, loss: 484.9007568359375\n",
      "Epoch #7, Metric loss: -7.136806488037109\n",
      "\n",
      "Epoch #8, Iteration #10, loss: 314.85589599609375\n",
      "Epoch #8, Iteration #20, loss: 613.129638671875\n",
      "Epoch #8, Iteration #30, loss: 269.81976318359375\n",
      "Epoch #8, Metric loss: -7.12629508972168\n",
      "\n",
      "Epoch #9, Iteration #10, loss: 345.403076171875\n",
      "Epoch #9, Iteration #20, loss: 266.3793029785156\n",
      "Epoch #9, Iteration #30, loss: 321.1495361328125\n",
      "Epoch #9, Metric loss: -7.2460832595825195\n",
      "\n",
      "Epoch #10, Iteration #10, loss: 430.03515625\n",
      "Epoch #10, Iteration #20, loss: 1052.08349609375\n",
      "Epoch #10, Iteration #30, loss: 233.6793670654297\n",
      "Epoch #10, Metric loss: -7.125190734863281\n",
      "\n",
      "***************** Fold No.[1] ********************\n",
      "Epoch #1, Iteration #10, loss: 4003.970458984375\n",
      "Epoch #1, Iteration #20, loss: 4256.3115234375\n",
      "Epoch #1, Iteration #30, loss: 3600.6044921875\n",
      "Epoch #1, Metric loss: -24.784719467163086\n",
      "\n",
      "Epoch #2, Iteration #10, loss: 3929.64453125\n",
      "Epoch #2, Iteration #20, loss: 3779.64111328125\n",
      "Epoch #2, Iteration #30, loss: 3525.795654296875\n",
      "Epoch #2, Metric loss: -24.429420471191406\n",
      "\n",
      "Epoch #3, Iteration #10, loss: 3331.65771484375\n",
      "Epoch #3, Iteration #20, loss: 2466.8662109375\n",
      "Epoch #3, Iteration #30, loss: 2230.88623046875\n",
      "Epoch #3, Metric loss: -17.880592346191406\n",
      "\n",
      "Epoch #4, Iteration #10, loss: 1364.352294921875\n",
      "Epoch #4, Iteration #20, loss: 1089.34814453125\n",
      "Epoch #4, Iteration #30, loss: 1001.1910400390625\n",
      "Epoch #4, Metric loss: -8.331154823303223\n",
      "\n",
      "Epoch #5, Iteration #10, loss: 459.1944885253906\n",
      "Epoch #5, Iteration #20, loss: 439.0762939453125\n",
      "Epoch #5, Iteration #30, loss: 630.2195434570312\n",
      "Epoch #5, Metric loss: -7.727484703063965\n",
      "\n",
      "Epoch #6, Iteration #10, loss: 491.19921875\n",
      "Epoch #6, Iteration #20, loss: 423.97344970703125\n",
      "Epoch #6, Iteration #30, loss: 399.9867248535156\n",
      "Epoch #6, Metric loss: -7.714046001434326\n",
      "\n",
      "Epoch #7, Iteration #10, loss: 383.58319091796875\n",
      "Epoch #7, Iteration #20, loss: 679.5926513671875\n",
      "Epoch #7, Iteration #30, loss: 496.03607177734375\n",
      "Epoch #7, Metric loss: -7.5983710289001465\n",
      "\n",
      "Epoch #8, Iteration #10, loss: 460.2842102050781\n",
      "Epoch #8, Iteration #20, loss: 516.8519287109375\n",
      "Epoch #8, Iteration #30, loss: 308.6128845214844\n",
      "Epoch #8, Metric loss: -7.626898288726807\n",
      "\n",
      "Epoch #9, Iteration #10, loss: 645.3373413085938\n",
      "Epoch #9, Iteration #20, loss: 279.6405944824219\n",
      "Epoch #9, Iteration #30, loss: 361.8935852050781\n",
      "Epoch #9, Metric loss: -7.624963283538818\n",
      "\n",
      "Epoch #10, Iteration #10, loss: 493.21649169921875\n",
      "Epoch #10, Iteration #20, loss: 293.3453369140625\n",
      "Epoch #10, Iteration #30, loss: 603.768310546875\n",
      "Epoch #10, Metric loss: -7.520409107208252\n",
      "\n",
      "***************** Fold No.[2] ********************\n",
      "Epoch #1, Iteration #10, loss: 3917.955810546875\n",
      "Epoch #1, Iteration #20, loss: 4270.591796875\n",
      "Epoch #1, Iteration #30, loss: 3685.31689453125\n",
      "Epoch #1, Metric loss: -24.798133850097656\n",
      "\n",
      "Epoch #2, Iteration #10, loss: 4327.7783203125\n",
      "Epoch #2, Iteration #20, loss: 4010.31005859375\n",
      "Epoch #2, Iteration #30, loss: 3550.71044921875\n",
      "Epoch #2, Metric loss: -24.420164108276367\n",
      "\n",
      "Epoch #3, Iteration #10, loss: 2997.897705078125\n",
      "Epoch #3, Iteration #20, loss: 2384.902099609375\n",
      "Epoch #3, Iteration #30, loss: 2293.7490234375\n",
      "Epoch #3, Metric loss: -16.908052444458008\n",
      "\n",
      "Epoch #4, Iteration #10, loss: 1024.955322265625\n",
      "Epoch #4, Iteration #20, loss: 1208.3897705078125\n",
      "Epoch #4, Iteration #30, loss: 675.5841674804688\n",
      "Epoch #4, Metric loss: -8.221291542053223\n",
      "\n",
      "Epoch #5, Iteration #10, loss: 600.6427001953125\n",
      "Epoch #5, Iteration #20, loss: 693.6812133789062\n",
      "Epoch #5, Iteration #30, loss: 511.97174072265625\n",
      "Epoch #5, Metric loss: -7.607140064239502\n",
      "\n",
      "Epoch #6, Iteration #10, loss: 662.2747802734375\n",
      "Epoch #6, Iteration #20, loss: 878.0948486328125\n",
      "Epoch #6, Iteration #30, loss: 451.0428161621094\n",
      "Epoch #6, Metric loss: -7.609993934631348\n",
      "\n",
      "Epoch #7, Iteration #10, loss: 487.2268371582031\n",
      "Epoch #7, Iteration #20, loss: 1043.0792236328125\n",
      "Epoch #7, Iteration #30, loss: 478.7058410644531\n",
      "Epoch #7, Metric loss: -7.465919017791748\n",
      "\n",
      "Epoch #8, Iteration #10, loss: 522.3629760742188\n",
      "Epoch #8, Iteration #20, loss: 898.7235107421875\n",
      "Epoch #8, Iteration #30, loss: 452.8639831542969\n",
      "Epoch #8, Metric loss: -7.396398067474365\n",
      "\n",
      "Epoch #9, Iteration #10, loss: 326.1005859375\n",
      "Epoch #9, Iteration #20, loss: 469.5611267089844\n",
      "Epoch #9, Iteration #30, loss: 422.6253662109375\n",
      "Epoch #9, Metric loss: -7.345789909362793\n",
      "\n",
      "Epoch #10, Iteration #10, loss: 559.9974365234375\n",
      "Epoch #10, Iteration #20, loss: 467.09375\n",
      "Epoch #10, Iteration #30, loss: 868.9505004882812\n",
      "Epoch #10, Metric loss: -7.390947341918945\n",
      "\n",
      "***************** Fold No.[3] ********************\n",
      "Epoch #1, Iteration #10, loss: 3818.81201171875\n",
      "Epoch #1, Iteration #20, loss: 4044.052001953125\n",
      "Epoch #1, Iteration #30, loss: 4199.9951171875\n",
      "Epoch #1, Metric loss: -24.720836639404297\n",
      "\n",
      "Epoch #2, Iteration #10, loss: 4106.1689453125\n",
      "Epoch #2, Iteration #20, loss: 4025.04150390625\n",
      "Epoch #2, Iteration #30, loss: 3784.3134765625\n",
      "Epoch #2, Metric loss: -24.265182495117188\n",
      "\n",
      "Epoch #3, Iteration #10, loss: 3028.382080078125\n",
      "Epoch #3, Iteration #20, loss: 2766.12890625\n",
      "Epoch #3, Iteration #30, loss: 1682.76025390625\n",
      "Epoch #3, Metric loss: -15.695205688476562\n",
      "\n",
      "Epoch #4, Iteration #10, loss: 935.4608154296875\n",
      "Epoch #4, Iteration #20, loss: 704.4400634765625\n",
      "Epoch #4, Iteration #30, loss: 568.9564819335938\n",
      "Epoch #4, Metric loss: -7.527125835418701\n",
      "\n",
      "Epoch #5, Iteration #10, loss: 1252.2369384765625\n",
      "Epoch #5, Iteration #20, loss: 315.54302978515625\n",
      "Epoch #5, Iteration #30, loss: 415.8680725097656\n",
      "Epoch #5, Metric loss: -7.352540016174316\n",
      "\n",
      "Epoch #6, Iteration #10, loss: 644.8173828125\n",
      "Epoch #6, Iteration #20, loss: 446.4203186035156\n",
      "Epoch #6, Iteration #30, loss: 445.32342529296875\n",
      "Epoch #6, Metric loss: -7.35251522064209\n",
      "\n",
      "Epoch #7, Iteration #10, loss: 482.60577392578125\n",
      "Epoch #7, Iteration #20, loss: 305.022216796875\n",
      "Epoch #7, Iteration #30, loss: 417.27490234375\n",
      "Epoch #7, Metric loss: -7.34832239151001\n",
      "\n",
      "Epoch #8, Iteration #10, loss: 646.0203247070312\n",
      "Epoch #8, Iteration #20, loss: 724.0436401367188\n",
      "Epoch #8, Iteration #30, loss: 258.177734375\n",
      "Epoch #8, Metric loss: -7.4232258796691895\n",
      "\n",
      "Epoch #9, Iteration #10, loss: 403.8947448730469\n",
      "Epoch #9, Iteration #20, loss: 347.1933898925781\n",
      "Epoch #9, Iteration #30, loss: 325.83355712890625\n",
      "Epoch #9, Metric loss: -7.334470272064209\n",
      "\n",
      "Epoch #10, Iteration #10, loss: 725.5291748046875\n",
      "Epoch #10, Iteration #20, loss: 293.6820068359375\n",
      "Epoch #10, Iteration #30, loss: 323.4529113769531\n",
      "Epoch #10, Metric loss: -7.333141803741455\n",
      "\n",
      "***************** Fold No.[4] ********************\n",
      "Epoch #1, Iteration #10, loss: 4225.18017578125\n",
      "Epoch #1, Iteration #20, loss: 4142.99365234375\n",
      "Epoch #1, Iteration #30, loss: 3980.3232421875\n",
      "Epoch #1, Metric loss: -24.768665313720703\n",
      "\n",
      "Epoch #2, Iteration #10, loss: 3637.245849609375\n",
      "Epoch #2, Iteration #20, loss: 3662.2685546875\n",
      "Epoch #2, Iteration #30, loss: 3618.84326171875\n",
      "Epoch #2, Metric loss: -23.936113357543945\n",
      "\n",
      "Epoch #3, Iteration #10, loss: 2735.324951171875\n",
      "Epoch #3, Iteration #20, loss: 2723.6376953125\n",
      "Epoch #3, Iteration #30, loss: 2327.119140625\n",
      "Epoch #3, Metric loss: -17.2427921295166\n",
      "\n",
      "Epoch #4, Iteration #10, loss: 1644.9583740234375\n",
      "Epoch #4, Iteration #20, loss: 1092.488525390625\n",
      "Epoch #4, Iteration #30, loss: 925.0387573242188\n",
      "Epoch #4, Metric loss: -9.38281536102295\n",
      "\n",
      "Epoch #5, Iteration #10, loss: 482.68463134765625\n",
      "Epoch #5, Iteration #20, loss: 305.476806640625\n",
      "Epoch #5, Iteration #30, loss: 322.4710998535156\n",
      "Epoch #5, Metric loss: -7.448178768157959\n",
      "\n",
      "Epoch #6, Iteration #10, loss: 286.3524169921875\n",
      "Epoch #6, Iteration #20, loss: 412.4286804199219\n",
      "Epoch #6, Iteration #30, loss: 280.8153076171875\n",
      "Epoch #6, Metric loss: -7.410604476928711\n",
      "\n",
      "Epoch #7, Iteration #10, loss: 370.1666564941406\n",
      "Epoch #7, Iteration #20, loss: 370.9959716796875\n",
      "Epoch #7, Iteration #30, loss: 1292.178466796875\n",
      "Epoch #7, Metric loss: -7.389454364776611\n",
      "\n",
      "Epoch #8, Iteration #10, loss: 411.39013671875\n",
      "Epoch #8, Iteration #20, loss: 849.7332153320312\n",
      "Epoch #8, Iteration #30, loss: 452.1239013671875\n",
      "Epoch #8, Metric loss: -7.613673686981201\n",
      "\n",
      "Epoch #9, Iteration #10, loss: 334.7442321777344\n",
      "Epoch #9, Iteration #20, loss: 506.91595458984375\n",
      "Epoch #9, Iteration #30, loss: 330.0616760253906\n",
      "Epoch #9, Metric loss: -7.358027935028076\n",
      "\n",
      "Epoch #10, Iteration #10, loss: 726.3240356445312\n",
      "Epoch #10, Iteration #20, loss: 352.21112060546875\n",
      "Epoch #10, Iteration #30, loss: 306.658203125\n",
      "Epoch #10, Metric loss: -7.409430980682373\n",
      "\n",
      "best score in fold 0: -7.125190734863281\n",
      "best score in fold 1: -7.520409107208252\n",
      "best score in fold 2: -7.345789909362793\n",
      "best score in fold 3: -7.333141803741455\n",
      "best score in fold 4: -7.358027935028076\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "data = MedicalDataset(mode='train')\n",
    "folds = data.group_kfold(num_kfolds)\n",
    "\n",
    "MAX = 10000000\n",
    "best_metric_score = [-10000] * num_kfolds\n",
    "for fold, (trainset, valset) in enumerate(folds):\n",
    "    print(\"***************** Fold No.{} ********************\".format([fold]))\n",
    "    \n",
    "    dataloaders = {\n",
    "        'train': DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2),\n",
    "        'val': DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    }\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = QrnnModel().to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        \n",
    "        all_pred = []\n",
    "        all_target = []\n",
    "        all_loss = []\n",
    "        all_metrix_loss = 0\n",
    "        \n",
    "        # train\n",
    "        itr = 1\n",
    "        for batch in dataloaders['train']:\n",
    "            model.train()\n",
    "            inputs = batch['features'].float().to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                preds = model(inputs)\n",
    "                loss = quantile_loss(preds, targets, quantiles)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                all_loss.append(loss)\n",
    "\n",
    "            if itr % 10 == 0:\n",
    "                print(f\"Epoch #{epoch+1}, Iteration #{itr}, loss: {loss}\")\n",
    "            itr += 1\n",
    "\n",
    "        # validation\n",
    "        for batch in dataloaders['val']:\n",
    "            model.eval()\n",
    "            inputs = batch['features'].float().to(device)\n",
    "            target = batch['target']\n",
    "            optimizer.zero_grad()\n",
    "            with torch.set_grad_enabled(False):\n",
    "                preds = model(inputs)\n",
    "                all_pred.extend(preds.detach().cpu().numpy().tolist())\n",
    "                all_target.extend(target.numpy().tolist())\n",
    "        \n",
    "        all_pred = torch.FloatTensor(all_pred)\n",
    "        all_target = torch.FloatTensor(all_target)\n",
    "        all_target=torch.reshape(all_target, all_pred[:,1].shape)\n",
    "        all_confidence = all_pred[:, 2] - all_pred[:, 0]\n",
    "        metric_loss = laplace_log_likelihood(all_pred[:, 1], all_target, all_confidence, True)\n",
    "\n",
    "        print(f\"Epoch #{epoch+1}, Metric loss: {metric_loss.mean()}\\n\")\n",
    "        best_metric_score[fold] = max(best_metric_score[fold], metric_loss.mean())\n",
    "            \n",
    "    models.append(model)\n",
    "\n",
    "for fold in range(num_kfolds):\n",
    "    print(f\"best score in fold {fold}: {best_metric_score[fold]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MedicalDataset(mode='test')\n",
    "avg_preds = np.zeros((len(data), len(quantiles)))\n",
    "                     \n",
    "for model in models:\n",
    "    dataloader = DataLoader(data, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    preds = []\n",
    "    for batch in dataloader:\n",
    "        inputs = batch['features'].float()\n",
    "        with torch.no_grad():\n",
    "            x = model(inputs)\n",
    "            preds.append(x)\n",
    "            \n",
    "    preds = torch.cat(preds, dim=0).numpy()\n",
    "    avg_preds += preds\n",
    "    \n",
    "avg_preds /= len(models)\n",
    "df = pd.DataFrame(data = avg_preds, columns=list(quantiles))\n",
    "df['Patient_Week'] = data.raw['Patient_Week']\n",
    "df['FVC'] = df[quantiles[1]]\n",
    "df['Confidence'] = df[quantiles[2]] - df[quantiles[0]]\n",
    "df = df.drop(columns=list(quantiles))\n",
    "df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient_Week</th>\n",
       "      <th>FVC</th>\n",
       "      <th>Confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID00419637202311204720264_-12</td>\n",
       "      <td>2842.916162</td>\n",
       "      <td>771.930811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID00419637202311204720264_-11</td>\n",
       "      <td>2840.654639</td>\n",
       "      <td>771.317822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID00419637202311204720264_-10</td>\n",
       "      <td>2838.393066</td>\n",
       "      <td>770.704639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID00419637202311204720264_-9</td>\n",
       "      <td>2836.131201</td>\n",
       "      <td>770.091162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID00419637202311204720264_-8</td>\n",
       "      <td>2833.869775</td>\n",
       "      <td>769.478174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Patient_Week          FVC  Confidence\n",
       "0  ID00419637202311204720264_-12  2842.916162  771.930811\n",
       "1  ID00419637202311204720264_-11  2840.654639  771.317822\n",
       "2  ID00419637202311204720264_-10  2838.393066  770.704639\n",
       "3   ID00419637202311204720264_-9  2836.131201  770.091162\n",
       "4   ID00419637202311204720264_-8  2833.869775  769.478174"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
