{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Import Libraries","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam\nfrom torch.utils.data import Dataset, DataLoader, Subset\n\nfrom sklearn.model_selection import GroupKFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_kfolds = 5\nbatch_size = 32\nlearning_rate = 3e-3\nnum_epoch = 10\nes_patience = 20\nquantiles = (0.2, 0.5, 0.8)\nmodel_name = 'descartes'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class MedicalDataset(Dataset):\n    def __init__(self, mode, transform=None):\n        self.transform = transform\n        self.mode = mode\n        \n        train = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/train.csv')\n        \n        # Some of the patients have inspected twice a week. So, remove duplicated records. \n        train.drop_duplicates(keep=False, inplace=True, subset=['Patient', 'Weeks'])\n        \n        sub = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/sample_submission.csv')\n        sub['Patient'] = sub['Patient_Week'].apply(lambda x: x.split('_')[0])\n        sub['Weeks'] = sub['Patient_Week'].apply(lambda x: int(x.split('_')[-1]))\n        sub = sub[['Patient', 'Weeks', 'Confidence', 'Patient_Week']]\n        \n        chunk = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/test.csv')\n        sub = sub.merge(chunk.drop('Weeks', axis=1), on='Patient')\n        \n        train['WHERE'] = 'train'\n        chunk['WHERE'] = 'val'\n        sub['WHERE'] = 'test'\n        \n        # Merge train, test, test data.\n        data = train.append([chunk, sub])\n        \n        # Calculate min_week by each patient.\n        data['min_week'] = data['Weeks']\n        data.loc[data.WHERE == 'test', 'min_week'] = np.nan\n        data['min_week'] = data.groupby('Patient')['min_week'].transform('min')\n        \n        init_exam = data.loc[data.Weeks == data.min_week]\n        init_exam = init_exam[['Patient', 'FVC']].copy()\n        init_exam.columns = ['Patient', 'min_FVC']\n        init_exam['nb'] = 1\n        init_exam['nb'] = init_exam.groupby('Patient')['nb'].transform('cumsum')\n        init_exam = init_exam[init_exam.nb == 1]\n        init_exam.drop('nb', axis=1, inplace=True)\n        \n        # Calculate day\n        data = data.merge(init_exam, on='Patient', how='left')\n        data['base_week'] = data['Weeks'] - data['min_week']\n        del init_exam\n        \n        # Transform 'Sex' and 'SmokingStatus' data into One-Hot vector.\n        COLS = ['Sex', 'SmokingStatus']\n        self.FE = []\n        for col in COLS:\n            for mod in data[col].unique():\n                self.FE.append(mod)\n                data[mod] = (data[col] == mod).astype(int)\n        \n        # Normalize each data so that the maximum value is 1 and the minimum value is 0.\n        data['age'] = (data['Age'] - data['Age'].min()) / \\\n                      (data['Age'].max() - data['Age'].min())\n        \n        data['BASE'] = (data['min_FVC'] - data['min_FVC'].min()) / \\\n                      (data['min_FVC'].max() - data['min_FVC'].min())\n\n        data['week'] = (data['base_week'] - data['base_week'].min()) / \\\n                      (data['base_week'].max() - data['base_week'].min())\n\n        data['percent'] = (data['Percent'] - data['Percent'].min()) / \\\n                      (data['Percent'].max() - data['Percent'].min())\n        \n        self.FE += ['age', 'percent', 'week', 'BASE']\n        self.raw = data.loc[data.WHERE == mode].reset_index()\n        del data\n        \n        \n    def __len__(self):\n        return len(self.raw)\n    \n    \n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n            \n        sample = {\n            'patient_id': self.raw['Patient'].iloc[idx],\n            'features': self.raw[self.FE].iloc[idx].values,\n            'target': self.raw['FVC'].iloc[idx]\n        }\n        if self.transform:\n            sample = transform(sample)\n    \n        return sample\n    \n    \n    def group_kfold(self, n_splits):\n        gkf = GroupKFold(n_splits=n_splits)\n        groups = self.raw['Patient']\n        for train_idx, val_idx in gkf.split(self.raw, self.raw, groups):\n            train = Subset(self, train_idx)\n            val = Subset(self, val_idx)\n            yield train, val\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create Model (Quantile Regression Neural Network)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class QrnnModel(nn.Module):\n    def __init__(self, in_tabular_features=9, out_quantiles=3):\n        super(QrnnModel, self).__init__()\n        self.fc1 = nn.Linear(in_tabular_features, 100)\n        self.bn1 = nn.BatchNorm1d(num_features=100)\n        self.fc2 = nn.Linear(100, 100)\n        self.fc3 = nn.Linear(100, out_quantiles)\n    \n    def forward(self, x):\n        x = F.leaky_relu(self.fc1(x))\n        x = self.bn1(x)\n        x = F.leaky_relu(self.fc2(x))\n        x = self.fc3(x)\n        \n        return x\n    \n\ndef quantile_loss(preds, target, quantiles):\n    losses = []\n    for i, q in enumerate(quantiles):\n        errors = target - preds[:, i]\n        losses.append(torch.max((q - 1) * errors, q * errors).unsqueeze(1))\n    loss = torch.mean(torch.sum(torch.cat(losses, dim=1), dim=1))\n    return loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"models = []\ndata = MedicalDataset(mode='train')\nfolds = data.group_kfold(num_kfolds)\n\nfor fold, (trainset, valset) in enumerate(folds):\n    \n    dataloaders = {\n        'train': DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2),\n        'val': DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=2)\n    }\n    \n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    model = QrnnModel().to(device)\n    optimizer = Adam(model.parameters(), lr=learning_rate)\n    \n    for epoch in range(num_epoch):\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()\n                \n            for batch in dataloaders[phase]:\n                inputs = batch['features'].float().to(device)\n                targets = batch['target'].to(device)\n\n                optimizer.zero_grad()\n\n                with torch.set_grad_enabled(phase == 'train'):\n                    preds = model(inputs)\n                    loss = quantile_loss(preds, targets, quantiles)\n\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                \n    models.append(model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predict Testdata","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = MedicalDataset(mode='test')\navg_preds = np.zeros((len(data), len(quantiles)))\n                     \nfor model in models:\n    dataloader = DataLoader(data, batch_size=batch_size, shuffle=False, num_workers=2)\n    \n    preds = []\n    for batch in dataloader:\n        inputs = batch['features'].float()\n        with torch.no_grad():\n            x = model(inputs)\n            preds.append(x)\n            \n    preds = torch.cat(preds, dim=0).numpy()\n    avg_preds += preds\n    \navg_preds /= len(models)\ndf = pd.DataFrame(data = avg_preds, columns=list(quantiles))\ndf['Patient_Week'] = data.raw['Patient_Week']\ndf['FVC'] = df[quantiles[1]]\ndf['Confidence'] = df[quantiles[2]] - df[quantiles[0]]\ndf = df.drop(columns=list(quantiles))\ndf.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}